# QM9 RAE Decoder Production Config
# Requires pretrained MAE checkpoint

output_dir: "data/runs/qm9_rae_decoder"
seed: 42

data:
  root: "data/datasets"
  dataset: "qm9"
  num_workers: 4

# Frozen MAE encoder
encoder:
  type: "mae"
  checkpoint: "data/runs/qm9_mae/checkpoints/best.pt"
  d_latent: 64  # Projection dimension for adapter

# Model dimensions (will be filled from data/encoder)
model:
  # These are filled dynamically from dataset
  # num_atom_types: filled from dataset
  # num_bond_types: filled from dataset

# RAE decoder architecture (following RAE paper recommendations)
rae_decoder:
  d_model: 512       # Wider than typical VAE decoder
  nhead: 8
  num_layers: 6      # Deeper than typical VAE decoder
  dim_feedforward: 2048  # DDT-style wide FFN
  dropout: 0.0       # No dropout (following diffusion conventions)

# Noise augmentation for diffusion robustness
noise_augmentation:
  enabled: true
  sigma_min: 0.0
  sigma_max: 1.0
  curriculum: true
  warmup_epochs: 50
  rampup_epochs: 100

# Loss weights
loss:
  lambda_node: 1.0
  lambda_edge: 1.0
  label_smoothing: 0.1
  focal_gamma: 2.0
  use_edge_class_weights: true

training:
  epochs: 200
  batch_size: 128
  lr: 1.0e-4
  weight_decay: 0.0
  max_grad_norm: 1.0
  warmup_epochs: 10
  use_scheduler: true
  min_lr: 1.0e-6
  save_every: 20
  eval_every: 20

wandb:
  enabled: true
  project: "molecule-rae"
