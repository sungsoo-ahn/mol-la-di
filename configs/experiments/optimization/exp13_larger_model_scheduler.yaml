# Experiment 13: Larger Model with Scheduler
# Goal: Better memorization with more capacity

output_dir: "data/optimization/exp13_larger_model_scheduler"
seed: 42
device: "cuda"

data:
  root: "data/datasets"
  dataset: "qm9"
  max_atoms: 9
  num_workers: 0
  debug: true
  debug_samples: 128

model:
  num_atom_types: 5
  num_bond_types: 5
  d_model: 512       # Larger
  nhead: 8
  num_layers: 8      # More layers
  dim_feedforward: 2048  # Larger
  dropout: 0.0

training:
  epochs: 10000
  batch_size: 128
  eval_batch_size: 128
  lr: 5.0e-4  # Lower LR for larger model
  min_lr: 1.0e-5
  weight_decay: 0.0
  max_grad_norm: 1.0
  warmup_epochs: 100  # Warmup for larger model
  use_scheduler: true
  save_every: 2500
  eval_every: 2500

eval:
  n_samples: 64
  final_n_samples: 128
  temperature: 1.0

wandb:
  enabled: false
