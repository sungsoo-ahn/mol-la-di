# QM9 Autoregressive Transformer Training Configuration

output_dir: "data/runs/qm9_ar_baseline"
seed: 42
device: "cuda"

# Data settings
data:
  root: "data/datasets"
  dataset: "qm9"
  max_atoms: 9  # Heavy atoms only (C, N, O, F)
  num_workers: 4

# Model architecture
model:
  num_atom_types: 5  # 0=empty, 1=C, 2=N, 3=O, 4=F (H added by RDKit)
  num_bond_types: 5  # none, single, double, triple, aromatic
  d_model: 256
  nhead: 8
  num_layers: 6
  dim_feedforward: 1024
  dropout: 0.1

# Training settings - optimized from medium benchmark experiments
training:
  epochs: 500  # Increased for full dataset convergence
  batch_size: 128
  eval_batch_size: 256
  lr: 1.0e-3  # Optimized: 10x higher LR with warmup
  weight_decay: 0.01
  max_grad_norm: 1.0  # Gradient clipping for stability
  warmup_epochs: 10  # Essential for higher LR
  use_scheduler: true
  min_lr: 1.0e-6
  save_every: 20
  eval_every: 10

# Evaluation settings
eval:
  n_samples: 1000
  final_n_samples: 10000
  temperature: 1.0

# Weights & Biases settings
wandb:
  enabled: true
  project: "molecule-generation"
  name: "qm9_ar_baseline"
